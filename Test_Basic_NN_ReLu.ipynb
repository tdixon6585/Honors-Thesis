{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_points = 3\n",
    "\n",
    "x = np.random.rand(num_data_points)*2 -1\n",
    "y_p = np.random.rand(num_data_points)*2 - 1\n",
    "\n",
    "X = np.array([(x[i], y_p[i]) for i in range(num_data_points)])\n",
    "y = []\n",
    "for index in range(num_data_points):\n",
    "    if y_p[index] > x[index]:\n",
    "        y.append([1])\n",
    "    else:\n",
    "        y.append([0])\n",
    "\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = y.T[0]\n",
    "c = np.where(c==0, 'red', c)\n",
    "c = np.where(c=='1', 'blue', c)\n",
    "plt.scatter(*zip(*X), c=c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node_class(object):\n",
    "    def __init__(self, input_dim, learning_rate=0.1):\n",
    "        #An array of weights, an additional weight is added for the dummy input `a_o = 1`\n",
    "        self.W = np.random.rand(int(input_dim+1))\n",
    "        self.learning_rate = learning_rate\n",
    "        print(\"Creating node with length \", len(self.W))\n",
    "        print(\"Weights \", self.W)\n",
    "        \n",
    "    def relu_activation(self, t):\n",
    "        self.final_output = np.max([0,t])\n",
    "        print(\"ReLu Activation! From \", t, \" to \", self.final_output)\n",
    "        return self.final_output\n",
    "    \n",
    "    def input_function(self, input_array):\n",
    "        print(\"Input function!\")\n",
    "        self.input_array = np.append(input_array,1)\n",
    "        self.input_sum = np.sum(self.input_array * self.W)\n",
    "        print(\"Sum of [input_array * weights]\")\n",
    "        print(\"Input array: \", self.input_array)\n",
    "        print(\"Weights: \", self.W)\n",
    "        print(\"Input sum: \", self.input_sum)\n",
    "        return self.input_sum\n",
    "    \n",
    "    #Consider removing\n",
    "    def output(self, input_array):\n",
    "        input_sum = self.input_function(input_array)\n",
    "        return self.relu_activation(input_sum)\n",
    "    \n",
    "    def relu_der(self,t):\n",
    "        if t>0:\n",
    "            print(\"ReLu Derivative! From \", t, \" to \", 1)\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"ReLu Derivative! From \", t, \" to \", 0)\n",
    "            return 0\n",
    "        \n",
    "    def calc_delta(self, actual_y=None, delta_js=None, weights=None, final_layer=False):\n",
    "        if final_layer:\n",
    "            print(\"Calc Delta @ Final Layer\")\n",
    "            print(\"Delta = relu_der[input_sum]*(actual_y-self.final_output)\")\n",
    "            print(\"Actual y: \", actual_y)\n",
    "            print(\"Final output: \", self.final_output)\n",
    "            self.delta = self.relu_der(self.input_sum)*(actual_y-self.final_output)\n",
    "            print(\"New Delta: \", self.delta)\n",
    "            return self.delta\n",
    "        else:\n",
    "            print(\"Calc Delta\")\n",
    "            print(\"Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\")\n",
    "            print(\"Weights: \", weights)\n",
    "            print(\"Deltas Js: \", delta_js)\n",
    "            self.delta = self.relu_der(self.input_sum) * np.sum(np.array(weights) * delta_js)\n",
    "            print(\"New Delta: \", self.delta)\n",
    "            return self.delta\n",
    "            \n",
    "    def update_weights(self):\n",
    "        print(\"Updating Weights\")\n",
    "        print(\"New Weights = self.W + (learning_rate * input_array * self.delta)\")\n",
    "        print(\"Old Weight: \", self.W)\n",
    "        print(\"Input Array: \", self.input_array)\n",
    "        print(\"Delta: \", self.delta)\n",
    "        self.W = self.W + (self.learning_rate * self.input_array * self.delta)\n",
    "        print(\"New Weights: \", self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        self.layers_nodes = []\n",
    "        print(\"Creating layers!\")\n",
    "        for i, num_nodes in enumerate(layers):\n",
    "            print(\"Layer \", i)\n",
    "            if i==0:\n",
    "                input_dim = num_nodes\n",
    "                continue\n",
    "                \n",
    "            dim = input_dim\n",
    "            self.layers_nodes.append([])\n",
    "            for j in range(num_nodes):\n",
    "                n = node_class(dim, learning_rate=learning_rate)\n",
    "                self.layers_nodes[i-1].append(n)\n",
    "                \n",
    "            input_dim = num_nodes\n",
    "            \n",
    "        \n",
    "            \n",
    "    def evaluate(self, predictions, y, categorical=True):\n",
    "        print(\"Evaluating!\")\n",
    "        if categorical:\n",
    "            evals = []\n",
    "            preds = np.array(predictions)\n",
    "            preds = np.round(preds)\n",
    "            for i in range(len(y)):\n",
    "                print(\"Comparison: \", preds[i], y[i])\n",
    "                if np.array_equal(preds[i], y[i]):\n",
    "                    evals.append(1)\n",
    "                    print(\"True!\")\n",
    "                else:\n",
    "                    evals.append(0)\n",
    "                    \n",
    "            evals = np.array(evals)\n",
    "            print(\"Acc: \", evals.mean())\n",
    "            return evals.mean()\n",
    "            \n",
    "    def predict(self, X, y):\n",
    "        preds = []\n",
    "        \n",
    "        print(\"Predicting!\")\n",
    "        for index, i in enumerate(X):\n",
    "            outputs = [i]\n",
    "            for h, layer in enumerate(self.layers_nodes):\n",
    "                outputs.append([])\n",
    "                for node in layer:\n",
    "                    output = node.output(outputs[h])\n",
    "                    outputs[h+1].append(output)\n",
    "                    \n",
    "            preds.append(outputs[-1])\n",
    "                        \n",
    "        return preds\n",
    "            \n",
    "    def train(self, X, y, epochs):\n",
    "        accs = []\n",
    "        \n",
    "        print(\"Training!\")\n",
    "        for j in range(epochs):\n",
    "            print(\"\\nEpoch \",j)\n",
    "            predictions = self.predict(X, y)\n",
    "            acc = self.evaluate(predictions,y)\n",
    "            accs.append(acc)\n",
    "            if j%100 == 0:\n",
    "                print('epoch ', j)\n",
    "                print('acc: ', acc)\n",
    "            for index, i in enumerate(X):\n",
    "                \n",
    "                outputs = [i]\n",
    "                print(\"\\nData point \", index)\n",
    "                print(\"x=\", i[0],\" y=\",i[1])\n",
    "                for h, layer in enumerate(self.layers_nodes):\n",
    "                    outputs.append([])\n",
    "                    print(\"\\nLayer \", h)\n",
    "                    for f, node in enumerate(layer):\n",
    "                        print(\"\\nNode \", f)\n",
    "                        output = node.output(outputs[h])\n",
    "                        outputs[h+1].append(output)\n",
    "                            \n",
    "                delta_js = []\n",
    "                layer_weights = []\n",
    "                print(\"\\n\\nCalculating deltas\")\n",
    "                print(\"Reversing the layers\")\n",
    "                for h, layer in enumerate(reversed(self.layers_nodes)):\n",
    "                    print(\"\\nLayer \", h)\n",
    "                    delta_js.append([])\n",
    "                    layer_weights.append([])\n",
    "                    for g, node in enumerate(layer):\n",
    "                        print(\"\\nNode \", g)\n",
    "                        if h == 0:\n",
    "                            delta_j = node.calc_delta(actual_y=y[index][g], final_layer=True)\n",
    "                            layer_weights[h].append(list(node.W))\n",
    "                            print(\"Adding layer_weights at layer \", h)\n",
    "                        else:\n",
    "                            weights = []\n",
    "                            for n in layer_weights[h-1]:\n",
    "                                weights.append(n[g])\n",
    "                            delta_j = node.calc_delta(delta_js=delta_js[h-1], weights=weights)\n",
    "                            layer_weights[h].append(list(node.W))\n",
    "                            print(\"Adding layer_weights at layer \", h)\n",
    "                            \n",
    "                        delta_js[h].append(delta_j)\n",
    "\n",
    "                        node.update_weights()\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating layers!\n",
      "Layer  0\n",
      "Layer  1\n",
      "Creating node with length  3\n",
      "Weights  [0.06703355 0.61803559 0.117702  ]\n",
      "Creating node with length  3\n",
      "Weights  [0.69847766 0.97446813 0.83166656]\n",
      "Creating node with length  3\n",
      "Weights  [0.65192809 0.08522441 0.43446792]\n",
      "Layer  2\n",
      "Creating node with length  4\n",
      "Weights  [0.34456486 0.27460302 0.10279573 0.78033839]\n"
     ]
    }
   ],
   "source": [
    "#Number of nodes for each layer\n",
    "#     First number is the input dimension\n",
    "#     Last number is the output dimension\n",
    "layers = [2,3,1]\n",
    "nn = NN(layers, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training!\n",
      "\n",
      "Epoch  0\n",
      "Predicting!\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.06703355 0.61803559 0.117702  ]\n",
      "Input sum:  0.11998375512149\n",
      "ReLu Activation! From  0.11998375512149  to  0.11998375512149\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.69847766 0.97446813 0.83166656]\n",
      "Input sum:  0.3247092843706012\n",
      "ReLu Activation! From  0.3247092843706012  to  0.3247092843706012\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.65192809 0.08522441 0.43446792]\n",
      "Input sum:  -0.11875024818202273\n",
      "ReLu Activation! From  -0.11875024818202273  to  0.0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.11998376 0.32470928 0.         1.        ]\n",
      "Weights:  [0.34456486 0.27460302 0.10279573 0.78033839]\n",
      "Input sum:  0.9108467329196474\n",
      "ReLu Activation! From  0.9108467329196474  to  0.9108467329196474\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.06703355 0.61803559 0.117702  ]\n",
      "Input sum:  -0.11551557881879629\n",
      "ReLu Activation! From  -0.11551557881879629  to  0.0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.69847766 0.97446813 0.83166656]\n",
      "Input sum:  0.3775099482304516\n",
      "ReLu Activation! From  0.3775099482304516  to  0.3775099482304516\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.65192809 0.08522441 0.43446792]\n",
      "Input sum:  0.3085938328199796\n",
      "ReLu Activation! From  0.3085938328199796  to  0.3085938328199796\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.         0.37750995 0.30859383 1.        ]\n",
      "Weights:  [0.34456486 0.27460302 0.10279573 0.78033839]\n",
      "Input sum:  0.9157258964033338\n",
      "ReLu Activation! From  0.9157258964033338  to  0.9157258964033338\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.06703355 0.61803559 0.117702  ]\n",
      "Input sum:  0.37936466656684853\n",
      "ReLu Activation! From  0.37936466656684853  to  0.37936466656684853\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.69847766 0.97446813 0.83166656]\n",
      "Input sum:  0.9395817237700901\n",
      "ReLu Activation! From  0.9395817237700901  to  0.9395817237700901\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.65192809 0.08522441 0.43446792]\n",
      "Input sum:  0.1402515544631196\n",
      "ReLu Activation! From  0.1402515544631196  to  0.1402515544631196\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.37936467 0.93958172 0.14025155 1.        ]\n",
      "Weights:  [0.34456486 0.27460302 0.10279573 0.78033839]\n",
      "Input sum:  1.1834833739541442\n",
      "ReLu Activation! From  1.1834833739541442  to  1.1834833739541442\n",
      "Evaluating!\n",
      "Comparison:  [1.] [1]\n",
      "True!\n",
      "Comparison:  [1.] [0]\n",
      "Comparison:  [1.] [1]\n",
      "True!\n",
      "Acc:  0.6666666666666666\n",
      "epoch  0\n",
      "acc:  0.6666666666666666\n",
      "\n",
      "Data point  0\n",
      "x= -0.8612823988989626  y= 0.09710860727847836\n",
      "\n",
      "Layer  0\n",
      "\n",
      "Node  0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.06703355 0.61803559 0.117702  ]\n",
      "Input sum:  0.11998375512149\n",
      "ReLu Activation! From  0.11998375512149  to  0.11998375512149\n",
      "\n",
      "Node  1\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.69847766 0.97446813 0.83166656]\n",
      "Input sum:  0.3247092843706012\n",
      "ReLu Activation! From  0.3247092843706012  to  0.3247092843706012\n",
      "\n",
      "Node  2\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.65192809 0.08522441 0.43446792]\n",
      "Input sum:  -0.11875024818202273\n",
      "ReLu Activation! From  -0.11875024818202273  to  0.0\n",
      "\n",
      "Layer  1\n",
      "\n",
      "Node  0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.11998376 0.32470928 0.         1.        ]\n",
      "Weights:  [0.34456486 0.27460302 0.10279573 0.78033839]\n",
      "Input sum:  0.9108467329196474\n",
      "ReLu Activation! From  0.9108467329196474  to  0.9108467329196474\n",
      "\n",
      "\n",
      "Calculating deltas\n",
      "Reversing the layers\n",
      "\n",
      "Layer  0\n",
      "\n",
      "Node  0\n",
      "Calc Delta @ Final Layer\n",
      "Delta = relu_der[input_sum]*(actual_y-self.final_output)\n",
      "Actual y:  1\n",
      "Final output:  0.9108467329196474\n",
      "ReLu Derivative! From  0.9108467329196474  to  1\n",
      "New Delta:  0.08915326708035265\n",
      "Adding layer_weights at layer  0\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.34456486 0.27460302 0.10279573 0.78033839]\n",
      "Input Array:  [0.11998376 0.32470928 0.         1.        ]\n",
      "Delta:  0.08915326708035265\n",
      "New Weights:  [0.34563456 0.27749791 0.10279573 0.78925372]\n",
      "\n",
      "Layer  1\n",
      "\n",
      "Node  0\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.3445648649393984]\n",
      "Deltas Js:  [0.08915326708035265]\n",
      "ReLu Derivative! From  0.11998375512149  to  1\n",
      "New Delta:  0.030719083430447824\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.06703355 0.61803559 0.117702  ]\n",
      "Input Array:  [-0.8612824   0.09710861  1.        ]\n",
      "Delta:  0.030719083430447824\n",
      "New Weights:  [0.06438777 0.6183339  0.1207739 ]\n",
      "\n",
      "Node  1\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.27460302458881936]\n",
      "Deltas Js:  [0.08915326708035265]\n",
      "ReLu Derivative! From  0.3247092843706012  to  1\n",
      "New Delta:  0.024481756792239656\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.69847766 0.97446813 0.83166656]\n",
      "Input Array:  [-0.8612824   0.09710861  1.        ]\n",
      "Delta:  0.024481756792239656\n",
      "New Weights:  [0.69636909 0.97470587 0.83411473]\n",
      "\n",
      "Node  2\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.1027957285340092]\n",
      "Deltas Js:  [0.08915326708035265]\n",
      "ReLu Derivative! From  -0.11875024818202273  to  0\n",
      "New Delta:  0.0\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.65192809 0.08522441 0.43446792]\n",
      "Input Array:  [-0.8612824   0.09710861  1.        ]\n",
      "Delta:  0.0\n",
      "New Weights:  [0.65192809 0.08522441 0.43446792]\n",
      "\n",
      "Data point  1\n",
      "x= -0.14581713011152675  y= -0.36153732580695075\n",
      "\n",
      "Layer  0\n",
      "\n",
      "Node  0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.06438777 0.6183339  0.1207739 ]\n",
      "Input sum:  -0.11216572008810828\n",
      "ReLu Activation! From  -0.11216572008810828  to  0.0\n",
      "\n",
      "Node  1\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.69636909 0.97470587 0.83411473]\n",
      "Input sum:  0.3801796381291996\n",
      "ReLu Activation! From  0.3801796381291996  to  0.3801796381291996\n",
      "\n",
      "Node  2\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.65192809 0.08522441 0.43446792]\n",
      "Input sum:  0.3085938328199796\n",
      "ReLu Activation! From  0.3085938328199796  to  0.3085938328199796\n",
      "\n",
      "Layer  1\n",
      "\n",
      "Node  0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.         0.38017964 0.30859383 1.        ]\n",
      "Weights:  [0.34563456 0.27749791 0.10279573 0.78925372]\n",
      "Input sum:  0.9264749060198001\n",
      "ReLu Activation! From  0.9264749060198001  to  0.9264749060198001\n",
      "\n",
      "\n",
      "Calculating deltas\n",
      "Reversing the layers\n",
      "\n",
      "Layer  0\n",
      "\n",
      "Node  0\n",
      "Calc Delta @ Final Layer\n",
      "Delta = relu_der[input_sum]*(actual_y-self.final_output)\n",
      "Actual y:  0\n",
      "Final output:  0.9264749060198001\n",
      "ReLu Derivative! From  0.9264749060198001  to  1\n",
      "New Delta:  -0.9264749060198001\n",
      "Adding layer_weights at layer  0\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.34563456 0.27749791 0.10279573 0.78925372]\n",
      "Input Array:  [0.         0.38017964 0.30859383 1.        ]\n",
      "Delta:  -0.9264749060198001\n",
      "New Weights:  [0.34563456 0.24227522 0.07420528 0.69660623]\n",
      "\n",
      "Layer  1\n",
      "\n",
      "Node  0\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.3456345593159634]\n",
      "Deltas Js:  [-0.9264749060198001]\n",
      "ReLu Derivative! From  -0.11216572008810828  to  0\n",
      "New Delta:  -0.0\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.06438777 0.6183339  0.1207739 ]\n",
      "Input Array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Delta:  -0.0\n",
      "New Weights:  [0.06438777 0.6183339  0.1207739 ]\n",
      "\n",
      "Node  1\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.2774979139441156]\n",
      "Deltas Js:  [-0.9264749060198001]\n",
      "ReLu Derivative! From  0.3801796381291996  to  1\n",
      "New Delta:  -0.2570948537420651\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.69636909 0.97470587 0.83411473]\n",
      "Input Array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Delta:  -0.2570948537420651\n",
      "New Weights:  [0.70011797 0.98400081 0.80840525]\n",
      "\n",
      "Node  2\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.1027957285340092]\n",
      "Deltas Js:  [-0.9264749060198001]\n",
      "ReLu Derivative! From  0.3085938328199796  to  1\n",
      "New Delta:  -0.09523766293278306\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.65192809 0.08522441 0.43446792]\n",
      "Input Array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Delta:  -0.09523766293278306\n",
      "New Weights:  [0.65331682 0.08866761 0.42494415]\n",
      "\n",
      "Data point  2\n",
      "x= -0.5139357278403647  y= 0.4791206379868913\n",
      "\n",
      "Layer  0\n",
      "\n",
      "Node  0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.06438777 0.6183339  0.1207739 ]\n",
      "Input sum:  0.3839392619557728\n",
      "ReLu Activation! From  0.3839392619557728  to  0.3839392619557728\n",
      "\n",
      "Node  1\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.70011797 0.98400081 0.80840525]\n",
      "Input sum:  0.9200447012824465\n",
      "ReLu Activation! From  0.9200447012824465  to  0.9200447012824465\n",
      "\n",
      "Node  2\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.65331682 0.08866761 0.42494415]\n",
      "Input sum:  0.13166377783832695\n",
      "ReLu Activation! From  0.13166377783832695  to  0.13166377783832695\n",
      "\n",
      "Layer  1\n",
      "\n",
      "Node  0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.38393926 0.9200447  0.13166378 1.        ]\n",
      "Weights:  [0.34563456 0.24227522 0.07420528 0.69660623]\n",
      "Input sum:  1.0619830932720815\n",
      "ReLu Activation! From  1.0619830932720815  to  1.0619830932720815\n",
      "\n",
      "\n",
      "Calculating deltas\n",
      "Reversing the layers\n",
      "\n",
      "Layer  0\n",
      "\n",
      "Node  0\n",
      "Calc Delta @ Final Layer\n",
      "Delta = relu_der[input_sum]*(actual_y-self.final_output)\n",
      "Actual y:  1\n",
      "Final output:  1.0619830932720815\n",
      "ReLu Derivative! From  1.0619830932720815  to  1\n",
      "New Delta:  -0.06198309327208151\n",
      "Adding layer_weights at layer  0\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.34563456 0.24227522 0.07420528 0.69660623]\n",
      "Input Array:  [0.38393926 0.9200447  0.13166378 1.        ]\n",
      "Delta:  -0.06198309327208151\n",
      "New Weights:  [0.34325479 0.2365725  0.07338919 0.69040792]\n",
      "\n",
      "Layer  1\n",
      "\n",
      "Node  0\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.3456345593159634]\n",
      "Deltas Js:  [-0.06198309327208151]\n",
      "ReLu Derivative! From  0.3839392619557728  to  1\n",
      "New Delta:  -0.02142349912813615\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.06438777 0.6183339  0.1207739 ]\n",
      "Input Array:  [-0.51393573  0.47912064  1.        ]\n",
      "Delta:  -0.02142349912813615\n",
      "New Weights:  [0.0654888  0.61730746 0.11863155]\n",
      "\n",
      "Node  1\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.24227522449347644]\n",
      "Deltas Js:  [-0.06198309327208151]\n",
      "ReLu Derivative! From  0.9200447012824465  to  1\n",
      "New Delta:  -0.015016967837293638\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.70011797 0.98400081 0.80840525]\n",
      "Input Array:  [-0.51393573  0.47912064  1.        ]\n",
      "Delta:  -0.015016967837293638\n",
      "New Weights:  [0.70088975 0.98328131 0.80690355]\n",
      "\n",
      "Node  2\n",
      "Calc Delta\n",
      "Delta = self.relu_der[input_sum] * SUM(weights * delta_js)\n",
      "Weights:  [0.07420528430799114]\n",
      "Deltas Js:  [-0.06198309327208151]\n",
      "ReLu Derivative! From  0.13166377783832695  to  1\n",
      "New Delta:  -0.004599473058543541\n",
      "Adding layer_weights at layer  1\n",
      "Updating Weights\n",
      "New Weights = self.W + (learning_rate * input_array * self.delta)\n",
      "Old Weight:  [0.65331682 0.08866761 0.42494415]\n",
      "Input Array:  [-0.51393573  0.47912064  1.        ]\n",
      "Delta:  -0.004599473058543541\n",
      "New Weights:  [0.65355321 0.08844724 0.42448421]\n"
     ]
    }
   ],
   "source": [
    "accuracies = nn.train(X,y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting!\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.0654888  0.61730746 0.11863155]\n",
      "Input sum:  0.12217307169714679\n",
      "ReLu Activation! From  0.12217307169714679  to  0.12217307169714679\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.70088975 0.98328131 0.80690355]\n",
      "Input sum:  0.2987246246093368\n",
      "ReLu Activation! From  0.2987246246093368  to  0.2987246246093368\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.8612824   0.09710861  1.        ]\n",
      "Weights:  [0.65355321 0.08844724 0.42448421]\n",
      "Input sum:  -0.1298206776002952\n",
      "ReLu Activation! From  -0.1298206776002952  to  0.0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.12217307 0.29872462 0.         1.        ]\n",
      "Weights:  [0.34325479 0.2365725  0.07338919 0.69040792]\n",
      "Input sum:  0.8030144452827993\n",
      "ReLu Activation! From  0.8030144452827993  to  0.8030144452827993\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.0654888  0.61730746 0.11863155]\n",
      "Input sum:  -0.11409752121980678\n",
      "ReLu Activation! From  -0.11409752121980678  to  0.0\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.70088975 0.98328131 0.80690355]\n",
      "Input sum:  0.3492089231159389\n",
      "ReLu Activation! From  0.3492089231159389  to  0.3492089231159389\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.14581713 -0.36153733  1.        ]\n",
      "Weights:  [0.65355321 0.08844724 0.42448421]\n",
      "Input sum:  0.29720797794302234\n",
      "ReLu Activation! From  0.29720797794302234  to  0.29720797794302234\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.         0.34920892 0.29720798 1.        ]\n",
      "Weights:  [0.34325479 0.2365725  0.07338919 0.69040792]\n",
      "Input sum:  0.7948330038798773\n",
      "ReLu Activation! From  0.7948330038798773  to  0.7948330038798773\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.0654888  0.61730746 0.11863155]\n",
      "Input sum:  0.3807392627739595\n",
      "ReLu Activation! From  0.3807392627739595  to  0.3807392627739595\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.70088975 0.98328131 0.80690355]\n",
      "Input sum:  0.9178016370423223\n",
      "ReLu Activation! From  0.9178016370423223  to  0.9178016370423223\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [-0.51393573  0.47912064  1.        ]\n",
      "Weights:  [0.65355321 0.08844724 0.42448421]\n",
      "Input sum:  0.1309767607485378\n",
      "ReLu Activation! From  0.1309767607485378  to  0.1309767607485378\n",
      "Input function!\n",
      "Sum of [input_array * weights]\n",
      "Input array:  [0.38073926 0.91780164 0.13097676 1.        ]\n",
      "Weights:  [0.34325479 0.2365725  0.07338919 0.69040792]\n",
      "Input sum:  1.0478374044678622\n",
      "ReLu Activation! From  1.0478374044678622  to  1.0478374044678622\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(X,y)\n",
    "preds = np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = np.array(preds).T\n",
    "c = np.where(c==0, 'red', c)\n",
    "c = np.where(c=='1.0', 'blue', c)\n",
    "plt.scatter(*zip(*X), c=c[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0654888  0.61730746 0.11863155]\n",
      "[0.70088975 0.98328131 0.80690355]\n",
      "[0.65355321 0.08844724 0.42448421]\n",
      "[0.34325479 0.2365725  0.07338919 0.69040792]\n"
     ]
    }
   ],
   "source": [
    "for layer in nn.layers_nodes:\n",
    "    for node in layer:\n",
    "        print(node.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa438729b10>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8612824 ,  0.09710861],\n",
       "       [-0.14581713, -0.36153733],\n",
       "       [-0.51393573,  0.47912064]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.concatenate((X,y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8612824 ,  0.09710861,  1.        ],\n",
       "       [-0.14581713, -0.36153733,  0.        ],\n",
       "       [-0.51393573,  0.47912064,  1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8612824 ,  0.09710861],\n",
       "       [-0.14581713, -0.36153733],\n",
       "       [-0.51393573,  0.47912064]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = l[:,:-1]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
